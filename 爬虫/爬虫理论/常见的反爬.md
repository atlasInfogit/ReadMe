#### 参考链接： https://blog.csdn.net/m0_37156322/article/details/84658872 

---

#### 1.字体加密

* 58同城字体加密

**出现问题**

​		字体加密一般是网页修改了默认的字符编码集，在网页上加载的他们自己定义的字体文件作为字体的样式，可以正确地显示数字，但是在源码上同样的二进制数由于未加载自定义的字体文件就由计算机默认编码成了乱码。 

**解决方案**

​		 一般来说，通用的解决办法是找到字体文件，分析文件中的映射关系。一般来说，字体文件都是作为样式加在加密字体的部位。 

```python
import base64
from io import BytesIO
from fontTools.ttLib import TTFont
import requests
import re
from lxml import etree

url = 'https://cd.58.com/wuhou/chuzu/b5j5'
res = requests.get(url)
bs64_str = re.findall("charset=utf-8;base64,(.*?)'\)", res.text)[0]


def get_page_show_ret(string):
    font = TTFont(BytesIO(base64.decodebytes(bs64_str.encode())))
    c = font.getBestCmap()
    ret_list = []
    for char in string:
        decode_num = ord(char)
        if decode_num in c:
            num = c[decode_num]
            num = int(num[-2:]) - 1
            ret_list.append(num)
        else:
            ret_list.append(char)
    ret_str_show = ''
    for num in ret_list:
        ret_str_show += str(num)
    return ret_str_show


page = etree.HTML(res.text)
li = page.xpath('.//ul[@class="house-list"]//li')[0:-1]
for each_li in li:
    title = each_li.xpath('.//div[@class="des"]/h2/a/text()')[0].strip()
    title = get_page_show_ret(title)
    price = each_li.xpath('.//div[@class="money"]/b/text()')[0]
    price = get_page_show_ret(price)
    print(title)
    print(price)
    print('=' * 50)
```



#### 2.ip限制

* 爬取一会出现身份验证，提示访问过多等

#### 3.常见反爬

* **通过headers中的关键字**
  * 反爬原理：爬虫默认情况下没有User-Agent
  * 解决方法：请求之前添加User-Agent即可；更好的方式是使用User-Agent池来解决（收集一堆User-Agent的方式，或者是随机生成User-Agent）
* **通过referer字段或者其他字段来反爬**
  * 反爬原理：爬虫默认情况下不会带上referer字段
  * 解决方法：添加referer字段
* **通过cookie判断**
  * 如果目标网站不需要登录 每次请求带上前一次返回的cookie，比如requests模块的session
  * 如果目标网站需要登录 准备多个账号，通过一个程序获取账号对应的cookie，组成cookie池，其他程序使用这些cookie
* **通过js来判断**
  * 通过js实现跳转来反爬
    * 反爬原理：js实现页面跳转，肉眼不可见
    * 解决方法: 在chrome中点击perserve log按钮实现观察页面跳转情况
  * 通过js生成了请求参数
    * 反爬原理：js生成了请求参数
    * 解决方法：分析js，观察加密的实现过程，通过js2py获取js的执行结果，或者使用selenium来实现
  * 通过js实现了数据加密
    * 反爬原理：js实现了数据的加密
    * 解决方法：分析js，观察加密的实现过程，通过js2py获取js的执行结果，或者使用selenium来实现

#### 4.通过验证码来反爬

* 反爬原理：对方服务器通过弹出验证码强制验证用户浏览行为
* 解决方法：打码平台或者是机器学习的方法识别验证码，其中打码平台廉价易用，更值得推荐

#### 5. 通过IP地址来反爬

* 反爬原理：正常浏览器请求网站，速度不会太快，同一个ip大量请求了对方服务器，有更大的可能性会被识别为爬虫
* 解决方法：对应的通过购买高质量的ip的方式能够解决问题

#### 6.总结

 反爬的手段非常多，但是一般而言，完全的模仿浏览器的行为即可 



